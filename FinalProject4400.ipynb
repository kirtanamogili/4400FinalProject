{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject4400.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirtanamogili/4400FinalProject/blob/main/FinalProject4400.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSIt-C2TKXK6",
        "outputId": "da54a2d9-95b0-4791-b181-1afa90361233"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd /content/gdrive/MyDrive/Colab\\ Notebooks\n",
        "!pip install python-Levenshtein scikit-learn pandas numpy\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "\n",
        "# 1. read data\n",
        "ltable = pd.read_csv(join('data', \"ltable.csv\"))\n",
        "rtable = pd.read_csv(join('data', \"rtable.csv\"))\n",
        "train = pd.read_csv(join('data', \"train.csv\"))\n",
        "\n",
        "# 2. blocking\n",
        "def pairs2LR(ltable, rtable, candset):\n",
        "    ltable.index = ltable.id\n",
        "    rtable.index = rtable.id\n",
        "    pairs = np.array(candset)\n",
        "    tpls_l = ltable.loc[pairs[:, 0], :]\n",
        "    tpls_r = rtable.loc[pairs[:, 1], :]\n",
        "    tpls_l.columns = [col + \"_l\" for col in tpls_l.columns]\n",
        "    tpls_r.columns = [col + \"_r\" for col in tpls_r.columns]\n",
        "    tpls_l.reset_index(inplace=True, drop=True)\n",
        "    tpls_r.reset_index(inplace=True, drop=True)\n",
        "    LR = pd.concat([tpls_l, tpls_r], axis=1)\n",
        "    return LR\n",
        "\n",
        "\n",
        "def block_by_brand(ltable, rtable):\n",
        "    # ensure brand is str\n",
        "    ltable['brand'] = ltable['brand'].astype(str)\n",
        "    rtable['brand'] = rtable['brand'].astype(str)\n",
        "\n",
        "    # get all brands\n",
        "    brands_l = set(ltable[\"brand\"].values)\n",
        "    brands_r = set(rtable[\"brand\"].values)\n",
        "    brands = brands_l.union(brands_r)\n",
        "\n",
        "    # map each brand to left ids and right ids\n",
        "    brand2ids_l = {b.lower(): [] for b in brands}\n",
        "    brand2ids_r = {b.lower(): [] for b in brands}\n",
        "    for i, x in ltable.iterrows():\n",
        "        brand2ids_l[x[\"brand\"].lower()].append(x[\"id\"])\n",
        "    for i, x in rtable.iterrows():\n",
        "        brand2ids_r[x[\"brand\"].lower()].append(x[\"id\"])\n",
        "\n",
        "    # put id pairs that share the same brand in candidate set\n",
        "    candset = []\n",
        "    for brd in brands:\n",
        "        l_ids = brand2ids_l[brd]\n",
        "        r_ids = brand2ids_r[brd]\n",
        "        for i in range(len(l_ids)):\n",
        "            for j in range(len(r_ids)):\n",
        "                candset.append([l_ids[i], r_ids[j]])\n",
        "    return candset\n",
        "\n",
        "# blocking to reduce the number of pairs to be compared\n",
        "candset = block_by_brand(ltable, rtable)\n",
        "print(\"number of pairs originally\", ltable.shape[0] * rtable.shape[0])\n",
        "print(\"number of pairs after blocking\",len(candset))\n",
        "candset_df = pairs2LR(ltable, rtable, candset)\n",
        "\n",
        "# 3. feature engineering\n",
        "import Levenshtein as lev\n",
        "\n",
        "def jaccard_similarity(row, attr):\n",
        "    x = set(row[attr + \"_l\"].lower().split())\n",
        "    y = set(row[attr + \"_r\"].lower().split())\n",
        "    return len(x.intersection(y)) / max(len(x), len(y))\n",
        "\n",
        "\n",
        "def levenshtein_distance(row, attr):\n",
        "    x = row[attr + \"_l\"].lower()\n",
        "    y = row[attr + \"_r\"].lower()\n",
        "    return lev.distance(x, y)\n",
        "\n",
        "def feature_engineering(LR):\n",
        "    LR = LR.astype(str)\n",
        "    attrs = [\"title\", \"category\", \"brand\", \"modelno\", \"price\"]\n",
        "    features = []\n",
        "    for attr in attrs:\n",
        "        j_sim = LR.apply(jaccard_similarity, attr=attr, axis=1)\n",
        "        l_dist = LR.apply(levenshtein_distance, attr=attr, axis=1)\n",
        "        features.append(j_sim)\n",
        "        features.append(l_dist)\n",
        "    features = np.array(features).T\n",
        "    return features\n",
        "candset_features = feature_engineering(candset_df)\n",
        "\n",
        "# also perform feature engineering to the training set\n",
        "training_pairs = list(map(tuple, train[[\"ltable_id\", \"rtable_id\"]].values))\n",
        "training_df = pairs2LR(ltable, rtable, training_pairs)\n",
        "training_features = feature_engineering(training_df)\n",
        "training_label = train.label.values\n",
        "\n",
        "# 4. model training and prediction using neural networks\n",
        "def neuralnet_training():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(15, input_dim=10, activation=\"relu\"))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(5, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def fit_model(training_features, training_label, candset_features, model):\n",
        "  model.fit(training_features, training_label, epochs=150, batch_size=10)\n",
        "  pred = model.predict_classes(candset_features)\n",
        "  return pred\n",
        "\n",
        "model = neuralnet_training()\n",
        "y_pred = fit_model(training_features,training_label, candset_features, model)\n",
        "\n",
        "# 5. output\n",
        "pairs = candset_df.loc[y_pred == 1, [\"id_l\", \"id_r\"]]\n",
        "pairs = list(map(tuple, pairs.values))\n",
        "pairs_training = training_df.loc[training_label == 1, [\"id_l\", \"id_r\"]]\n",
        "pairs_training = set(list(map(tuple, pairs_training.values)))\n",
        "\n",
        "p_pairs = [pair for pair in pairs if\n",
        "              pair not in pairs_training]\n",
        "              \n",
        "p_pairs = np.array(p_pairs)\n",
        "pred_df = pd.DataFrame(p_pairs, columns=[\"ltable_id\", \"rtable_id\"])\n",
        "pred_df.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (56.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "number of pairs originally 56376996\n",
            "number of pairs after blocking 256606\n",
            "Epoch 1/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.5547 - accuracy: 0.8516\n",
            "Epoch 2/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2329 - accuracy: 0.9280\n",
            "Epoch 3/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2421 - accuracy: 0.9277\n",
            "Epoch 4/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2414 - accuracy: 0.9354\n",
            "Epoch 5/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2127 - accuracy: 0.9421\n",
            "Epoch 6/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1996 - accuracy: 0.9477\n",
            "Epoch 7/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2257 - accuracy: 0.9390\n",
            "Epoch 8/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1929 - accuracy: 0.9495\n",
            "Epoch 9/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2148 - accuracy: 0.9429\n",
            "Epoch 10/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2364 - accuracy: 0.9355\n",
            "Epoch 11/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1971 - accuracy: 0.9501\n",
            "Epoch 12/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2046 - accuracy: 0.9390\n",
            "Epoch 13/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1974 - accuracy: 0.9468\n",
            "Epoch 14/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1965 - accuracy: 0.9504\n",
            "Epoch 15/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2059 - accuracy: 0.9427\n",
            "Epoch 16/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1915 - accuracy: 0.9476\n",
            "Epoch 17/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1945 - accuracy: 0.9468\n",
            "Epoch 18/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2099 - accuracy: 0.9411\n",
            "Epoch 19/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1920 - accuracy: 0.9424\n",
            "Epoch 20/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1948 - accuracy: 0.9486\n",
            "Epoch 21/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.9487\n",
            "Epoch 22/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1921 - accuracy: 0.9448\n",
            "Epoch 23/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2025 - accuracy: 0.9424\n",
            "Epoch 24/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.9479\n",
            "Epoch 25/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2018 - accuracy: 0.9445\n",
            "Epoch 26/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.9507\n",
            "Epoch 27/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1864 - accuracy: 0.9477\n",
            "Epoch 28/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1863 - accuracy: 0.9469\n",
            "Epoch 29/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1902 - accuracy: 0.9483\n",
            "Epoch 30/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1764 - accuracy: 0.9497\n",
            "Epoch 31/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1951 - accuracy: 0.9441\n",
            "Epoch 32/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.9506\n",
            "Epoch 33/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.9488\n",
            "Epoch 34/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.9485\n",
            "Epoch 35/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1753 - accuracy: 0.9526\n",
            "Epoch 36/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1817 - accuracy: 0.9488\n",
            "Epoch 37/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2041 - accuracy: 0.9399\n",
            "Epoch 38/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.9458\n",
            "Epoch 39/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1925 - accuracy: 0.9445\n",
            "Epoch 40/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.9517\n",
            "Epoch 41/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1844 - accuracy: 0.9461\n",
            "Epoch 42/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.9502\n",
            "Epoch 43/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1944 - accuracy: 0.9423\n",
            "Epoch 44/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1719 - accuracy: 0.9499\n",
            "Epoch 45/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.9480\n",
            "Epoch 46/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.9465\n",
            "Epoch 47/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1746 - accuracy: 0.9515\n",
            "Epoch 48/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.2007 - accuracy: 0.9396\n",
            "Epoch 49/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1967 - accuracy: 0.9433\n",
            "Epoch 50/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1744 - accuracy: 0.9502\n",
            "Epoch 51/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1682 - accuracy: 0.9537\n",
            "Epoch 52/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1851 - accuracy: 0.9448\n",
            "Epoch 53/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.9506\n",
            "Epoch 54/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1772 - accuracy: 0.9463\n",
            "Epoch 55/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1769 - accuracy: 0.9495\n",
            "Epoch 56/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1633 - accuracy: 0.9544\n",
            "Epoch 57/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1706 - accuracy: 0.9541\n",
            "Epoch 58/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1667 - accuracy: 0.9507\n",
            "Epoch 59/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1699 - accuracy: 0.9516\n",
            "Epoch 60/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1943 - accuracy: 0.9431\n",
            "Epoch 61/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.9459\n",
            "Epoch 62/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1697 - accuracy: 0.9525\n",
            "Epoch 63/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.9468\n",
            "Epoch 64/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.9488\n",
            "Epoch 65/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1735 - accuracy: 0.9501\n",
            "Epoch 66/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1879 - accuracy: 0.9421\n",
            "Epoch 67/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1713 - accuracy: 0.9491\n",
            "Epoch 68/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1693 - accuracy: 0.9528\n",
            "Epoch 69/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1714 - accuracy: 0.9523\n",
            "Epoch 70/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1822 - accuracy: 0.9463\n",
            "Epoch 71/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1725 - accuracy: 0.9493\n",
            "Epoch 72/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1715 - accuracy: 0.9504\n",
            "Epoch 73/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1771 - accuracy: 0.9470\n",
            "Epoch 74/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.9466\n",
            "Epoch 75/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.9442\n",
            "Epoch 76/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.9480\n",
            "Epoch 77/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.9463\n",
            "Epoch 78/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1747 - accuracy: 0.9486\n",
            "Epoch 79/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1782 - accuracy: 0.9485\n",
            "Epoch 80/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1712 - accuracy: 0.9496\n",
            "Epoch 81/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1827 - accuracy: 0.9458\n",
            "Epoch 82/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1741 - accuracy: 0.9438\n",
            "Epoch 83/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1660 - accuracy: 0.9494\n",
            "Epoch 84/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1655 - accuracy: 0.9534\n",
            "Epoch 85/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1693 - accuracy: 0.9485\n",
            "Epoch 86/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1656 - accuracy: 0.9512\n",
            "Epoch 87/150\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.9532\n",
            "Epoch 88/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1594 - accuracy: 0.9544\n",
            "Epoch 89/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1658 - accuracy: 0.9506\n",
            "Epoch 90/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1721 - accuracy: 0.9509\n",
            "Epoch 91/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1613 - accuracy: 0.9524\n",
            "Epoch 92/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1672 - accuracy: 0.9529\n",
            "Epoch 93/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1660 - accuracy: 0.9545\n",
            "Epoch 94/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.9470\n",
            "Epoch 95/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1694 - accuracy: 0.9549\n",
            "Epoch 96/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1713 - accuracy: 0.9514\n",
            "Epoch 97/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1653 - accuracy: 0.9539\n",
            "Epoch 98/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1810 - accuracy: 0.9467\n",
            "Epoch 99/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1656 - accuracy: 0.9558\n",
            "Epoch 100/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1659 - accuracy: 0.9510\n",
            "Epoch 101/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.9474\n",
            "Epoch 102/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1704 - accuracy: 0.9521\n",
            "Epoch 103/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1581 - accuracy: 0.9555\n",
            "Epoch 104/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1637 - accuracy: 0.9534\n",
            "Epoch 105/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1728 - accuracy: 0.9489\n",
            "Epoch 106/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1707 - accuracy: 0.9473\n",
            "Epoch 107/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1598 - accuracy: 0.9541\n",
            "Epoch 108/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1708 - accuracy: 0.9510\n",
            "Epoch 109/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1706 - accuracy: 0.9521\n",
            "Epoch 110/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1641 - accuracy: 0.9551\n",
            "Epoch 111/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1641 - accuracy: 0.9541\n",
            "Epoch 112/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1594 - accuracy: 0.9542\n",
            "Epoch 113/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1727 - accuracy: 0.9484\n",
            "Epoch 114/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1701 - accuracy: 0.9498\n",
            "Epoch 115/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1607 - accuracy: 0.9551\n",
            "Epoch 116/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1609 - accuracy: 0.9544\n",
            "Epoch 117/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1611 - accuracy: 0.9555\n",
            "Epoch 118/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1734 - accuracy: 0.9502\n",
            "Epoch 119/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1742 - accuracy: 0.9510\n",
            "Epoch 120/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1721 - accuracy: 0.9490\n",
            "Epoch 121/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1823 - accuracy: 0.9489\n",
            "Epoch 122/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1678 - accuracy: 0.9514\n",
            "Epoch 123/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1646 - accuracy: 0.9520\n",
            "Epoch 124/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.9463\n",
            "Epoch 125/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1750 - accuracy: 0.9473\n",
            "Epoch 126/150\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9517\n",
            "Epoch 127/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1691 - accuracy: 0.9506\n",
            "Epoch 128/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1606 - accuracy: 0.9523\n",
            "Epoch 129/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1640 - accuracy: 0.9567\n",
            "Epoch 130/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1710 - accuracy: 0.9506\n",
            "Epoch 131/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1684 - accuracy: 0.9523\n",
            "Epoch 132/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1494 - accuracy: 0.9581\n",
            "Epoch 133/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1664 - accuracy: 0.9519\n",
            "Epoch 134/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1459 - accuracy: 0.9602\n",
            "Epoch 135/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1494 - accuracy: 0.9588\n",
            "Epoch 136/150\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.1594 - accuracy: 0.9553\n",
            "Epoch 137/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1509 - accuracy: 0.9579\n",
            "Epoch 138/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1626 - accuracy: 0.9542\n",
            "Epoch 139/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.9484\n",
            "Epoch 140/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1588 - accuracy: 0.9541\n",
            "Epoch 141/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1642 - accuracy: 0.9524\n",
            "Epoch 142/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1628 - accuracy: 0.9541\n",
            "Epoch 143/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1575 - accuracy: 0.9562\n",
            "Epoch 144/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1621 - accuracy: 0.9553\n",
            "Epoch 145/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1690 - accuracy: 0.9515\n",
            "Epoch 146/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1540 - accuracy: 0.9551\n",
            "Epoch 147/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1584 - accuracy: 0.9538\n",
            "Epoch 148/150\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.1514 - accuracy: 0.9568\n",
            "Epoch 149/150\n",
            "500/500 [==============================] - 1s 2ms/step - loss: 0.1557 - accuracy: 0.9571\n",
            "Epoch 150/150\n",
            "500/500 [==============================] - 1s 1ms/step - loss: 0.1617 - accuracy: 0.9548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mruqnD7pRwOP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}